{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtMfHpe1iOSD",
        "outputId": "c02548c0-aad7-4354-b002-c4e86a47ae45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Error loading word_tokenize: Package 'word_tokenize' not\n",
            "[nltk_data]     found in index\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"word_tokenize\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "def data_cleaning(review):\n",
        "    review = review.lower()\n",
        "    review = re.sub(r'\\W', ' ', review)\n",
        "    review = re.sub(r'\\s+', ' ', review)\n",
        "    review = re.sub(r'[0-9]+', ' ', review)\n",
        "    words = nltk.word_tokenize(review)\n",
        "    new_words = ' '.join(lemmatizer.lemmatize(word) for word in words if word not in stopwords.words('english'))\n",
        "    return new_words\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "train_data = pandas.read_table(\"train_file.dat\", sep=\"\\t\", names = ['Point', 'Review'])\n",
        "train_data['Review'].fillna('', inplace = True)\n",
        "with open(\"1675140109_010778_1567602457_126649_test.dat\") as file:\n",
        "    test = file.read()\n",
        "test_data = pandas.DataFrame(test.splitlines(), columns = ['Comment'])\n",
        "\n",
        "\n",
        "train_data['Review'] = train_data['Review'].apply(data_cleaning)\n",
        "test_data['Comment'] = test_data['Comment'].apply(data_cleaning)\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(train_data[\"Review\"], train_data[\"Point\"], test_size=0.25, random_state=0)\n",
        "\n",
        "\n",
        "tfidf_vec = TfidfVectorizer(min_df = 10, token_pattern = r'[a-zA-Z]+')\n",
        "count_vec = CountVectorizer(min_df = 10, token_pattern = r'[a-zA-Z]+')\n",
        "\n",
        "\n",
        "# with Count Vectorization\n",
        "X_train_bow = count_vec.fit_transform(x_train)\n",
        "#X_test_bow = count_vec.transform(x_test)\n",
        "X_test_bow = count_vec.transform(test_data['Comment'])\n",
        "model_lg = LogisticRegression()\n",
        "model_lg.fit(X_train_bow, y_train)\n",
        "predict = model_lg.predict(X_test_bow)\n",
        "#print(\"Accuracy Using Count Vectorization: \", str(metrics.accuracy_score(y_test,predict)))\n",
        "\n",
        "\n",
        "# with TF-IDF\n",
        "X_train_bow = tfidf_vec.fit_transform(x_train)\n",
        "#X_test_bow = tfidf_vec.transform(x_test)\n",
        "X_test_bow = tfidf_vec.transform(test_data['Comment'])\n",
        "model_lg = LogisticRegression()\n",
        "model_lg.fit(X_train_bow, y_train)\n",
        "predict = model_lg.predict(X_test_bow)\n",
        "#print(\"Accuracy Using TF-IDF: \", str(metrics.accuracy_score(y_test,predict)))\n",
        "\n",
        "\n",
        "final_predictions = pandas.DataFrame({\"Ratings\":predict})\n",
        "final_predictions[\"Ratings\"].to_csv(\"Final.csv\", header = False, index = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZfvN96TnBtD1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}